{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85723,"databundleVersionId":10652996,"sourceType":"competition"},{"sourceId":10441952,"sourceType":"datasetVersion","datasetId":6463074}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install optuna-integration[catboost]\n!pip install -U holidays","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U pycountry","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import statsmodels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport optuna\nimport catboost as cb\nimport matplotlib\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.model_selection import train_test_split\nfrom optuna.integration import CatBoostPruningCallback\nimport pycountry \nimport holidays\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train=pd.read_csv('/kaggle/input/playground-series-s5e1/train.csv')\ndf_test=pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['store'].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['product'].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport matplotlib.pyplot as plt\n\n# Generate synthetic time series data\nnp.random.seed(42)\n#df_train['date']=pd.to_datetime(df_train['date'])\n#df_train=df_train.set_index('date')\ndf=df_train['num_sold']\n\n#time = pd.date_range(start=df_train['date'].min, end=df_train['date'].max, periods=365, freq='D')\n#series = 10 + 0.05 * np.arange(100) + np.sin(2 * np.pi * time.dayofyear / 365) + np.random.normal(0, 1, 100)\n#df = pd.DataFrame({'date': time, 'value': series}).set_index('date')\n\n# Decompose time series\ndecomposition = seasonal_decompose(df_train[(df_train['country']=='Singapore') & (df_train['store']=='Discount Stickers')]['num_sold'], model='multiplicative', period=600)  # Set period according to data frequency\n\n# Plot components\ndecomposition.plot()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\n# Perform ADF test\nadf_result = adfuller(df_train[(df_train['country']=='Singapore') & (df_train['store']=='Discount Stickers')]['num_sold'])\n\n# Print results\nprint(\"ADF Statistic:\", adf_result[0])\nprint(\"p-value:\", adf_result[1])\nif adf_result[1] < 0.05:\n    print(\"The series is stationary.\")\nelse:\n    print(\"The series is not stationary.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from statsmodels.tsa.stattools import kpss\n\n# Perform KPSS test\nkpss_result = kpss(df_train[(df_train['country']=='Singapore') & (df_train['store']=='Discount Stickers')]['num_sold'], regression='c')\n\n# Print results\nprint(\"KPSS Statistic:\", kpss_result[0])\nprint(\"p-value:\", kpss_result[1])\nif kpss_result[1] < 0.05:\n    print(\"The series is not stationary (has a trend).\")\nelse:\n    print(\"The series is stationary.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot showing timeseries attributes: trend, seasonality \n\n# Create subplots: 2 rows, 3 columns\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\n\n# Flatten the axes array for easy iteration\naxes = axes.flatten()\ncountrylist=df_train['country'].unique().tolist()\n# Loop through each subplot and plot\nfor i, ax in enumerate(axes):\n    df=df_train[(df_train['country']==countrylist[i]) & (df_train['store']=='Discount Stickers')][['date','num_sold']]\n    df.dropna(inplace=True)\n    df.set_index('date',inplace=True)\n    df['rolling_mean'] = df['num_sold'].rolling(window=30).mean()\n    df['rolling_std'] = df['num_sold'].rolling(window=30).std()\n    df[['num_sold', 'rolling_mean', 'rolling_std']].plot(ax=ax,title=f'{countrylist[i]} Rolling Mean and Std Dev')\n\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train[(df_train['country']=='Singapore') & (df_train['store']=='Discount Sticker')]['num_sold']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef get_alpha2_code(country_name):\n  \"\"\"\n  Gets the ISO 3166-1 Alpha-2 country code for the given country name.\n\n  Args:\n    country_name: The name of the country.\n\n  Returns:\n    The ISO 3166-1 Alpha-2 country code, or None if the country is not found.\n  \"\"\"\n  try:\n    country = pycountry.countries.search_fuzzy(country_name)[0]\n    return country.alpha_2\n  except LookupError:\n    return None\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to create holiday sets for each country\ndef generate_country_holidays(countries, start_year, end_year):\n    country_holidays = {}\n    for country in countries:\n        # Generate a holiday set for the range of years\n        country_holidays[country] = holidays.CountryHoliday(country, years=range(start_year, end_year + 1)) \n    return country_holidays\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def consecutive_days_visited(df):\n    \"\"\"\n    Calculates the number of consecutive days a store was visited.\n\n    Args:\n        df: pandas DataFrame with columns 'date', 'country', 'store'\n\n    Returns:\n        pandas DataFrame with an additional column 'consecutive_days' \n    \"\"\"\n\n    # Ensure date index is sorted\n    df = df.sort_index()\n\n    # Group by 'country' and 'store'\n    grouped = df.groupby(['country', 'store'])\n\n    # Calculate consecutive days within each group\n    df['consecutive_days'] = (grouped.indices - grouped.indices.min()).dt.days + 1\n\n    return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"allcountries=df_train['country'].unique()\ncountrycodes = list(map(lambda x:get_alpha2_code(x),allcountries)) \nunique_countries = [cc for cc in countrycodes if cc is not None]\nprint(countrycodes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['Day']=pd.to_datetime(df_train['date']).dt.strftime(\"%d\")\ndf_train['Month']=pd.to_datetime(df_train['date']).dt.strftime(\"%m\")\ndf_train['Year']=pd.to_datetime(df_train['date']).dt.strftime(\"%Y\")\ndf_train['Day_of_week']=pd.to_datetime(df_train['date']).dt.dayofweek\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['store']=df_train['store'].astype('category')\ndf_train['country']=df_train['country'].astype('category')\ndf_train['product']=df_train['product'].astype('category')\ndf_train['Day']=df_train['Day'].astype('category')\ndf_train['Month']=df_train['Month'].astype('category')\ndf_train['Year']=df_train['Year'].astype('category')\ndf_train['Day_of_week']=df_train['Day_of_week'].astype('category')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['Day']=pd.to_datetime(df_test['date']).dt.strftime(\"%d\")\ndf_test['Month']=pd.to_datetime(df_test['date']).dt.strftime(\"%m\")\ndf_test['Year']=pd.to_datetime(df_test['date']).dt.strftime(\"%Y\")\ndf_test['Day_of_week']=pd.to_datetime(df_test['date']).dt.dayofweek","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['store']=df_test['store'].astype('category')\ndf_test['country']=df_test['country'].astype('category')\ndf_test['product']=df_test['product'].astype('category')\ndf_test['Day']=df_test['Day'].astype('category')\ndf_test['Month']=df_test['Month'].astype('category')\ndf_test['Year']=df_test['Year'].astype('category')\ndf_test['Day_of_week']=df_test['Day_of_week'].astype('category')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['date']=pd.to_datetime(df_train['date'])\ndf_test['date']=pd.to_datetime(df_test['date'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['previous_visit']=df_train.reset_index().groupby(by=['country','store'])[['date']].diff()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['consec_days']=df_train.groupby(by=['country','store'])[['previous_visit']].cumsum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['previous_visit']=df_test.reset_index().groupby(by=['country','store'])[['date']].diff()\ndf_test['consec_days']=df_test.groupby(by=['country','store'])[['previous_visit']].cumsum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.set_index('date',inplace=True)\nStartmin=df_train.index.min()\ndf_train['days_since_start'] = (df_train.index - Startmin).days\n\n# Add cycle hints based on time \ndf_train['sin_year'] = np.sin(2 * np.pi * df_train['days_since_start'] / 365)\ndf_train['cos_year'] = np.cos(2 * np.pi * df_train['days_since_start'] / 365)\ndf_train['sin_month'] = np.sin(2 * np.pi * df_train['days_since_start'] / 30)\ndf_train['cos_month'] = np.cos(2 * np.pi * df_train['days_since_start'] / 30)\n\ndf_test.set_index('date',inplace=True)\ndf_test['days_since_start'] = (df_test.index - Startmin).days\n\n# Add cycle hints based on time \ndf_test['sin_year'] = np.sin(2 * np.pi * df_test['days_since_start'] / 365)\ndf_test['cos_year'] = np.cos(2 * np.pi * df_test['days_since_start'] / 365)\ndf_test['sin_month'] = np.sin(2 * np.pi * df_test['days_since_start'] / 30)\ndf_test['cos_month'] = np.cos(2 * np.pi * df_test['days_since_start'] / 30)\n\ndf_train.reset_index(inplace=True)\ndf_test.reset_index(inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train[(df_train['country']=='Norway') & (df_train['store']=='Discount Stickers') & (df_train['previous_visit'].astype('int')>0)].consec_days.max","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Get country\n\n#Get holidays for each country\n#allcountries.append(\"weirdcountry\")\ncountry_holidays={}\n\n#df_train['date']=pd.to_datetime(df_train['date'])\nstart_year = df_train['date'].dt.year.min()\nend_year = df_train['date'].dt.year.max()\n\ncountry_holidays = generate_country_holidays(unique_countries, start_year, end_year)\nholiday_df = pd.DataFrame([\n    {'date': date, 'country_code': country, 'country': cname}\n    for country in unique_countries\n    for date in country_holidays[country]\n    for cname in [pycountry.countries.get(alpha_2=cc).name for cc in unique_countries]]\n)\nholiday_df['is_holiday'] = 1\nholiday_df['date']=pd.to_datetime(holiday_df['date'])\nholiday_df.drop('country_code',axis=1,inplace=True)\n\ndf_train = df_train.merge(holiday_df, on=['date', 'country'], how='left').fillna({'is_holiday': 0})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_year = df_test['date'].dt.year.min()\nend_year = df_test['date'].dt.year.max()\n#unique_countries = [cc for cc in countrycodes if cc is not None]\ncountry_holidays = generate_country_holidays(unique_countries, start_year, end_year)\n\nholiday_df = pd.DataFrame([\n    {'date': date, 'country_code': country, 'country': cname}\n    for country in unique_countries\n    for date in country_holidays[country]\n    for cname in [pycountry.countries.get(alpha_2=cc).name for cc in unique_countries]]\n)\nholiday_df['is_holiday']=1\nholiday_df['date']=pd.to_datetime(holiday_df['date'])\nholiday_df.drop('country_code',axis=1,inplace=True)\n\ndf_test = df_test.merge(holiday_df, on=['date', 'country'], how='left').fillna({'is_holiday': 0})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['is_holiday']=df_test['is_holiday'].astype(int)\ndf_test['is_holiday'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.set_index('date',inplace=True)\n#df_test.set_index('date',inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Graph for intrpolate is changing too much\n#Not interpolate but just drop Na is better\n#Interpolate missing num_sold\n\n#df_train['date']=pd.to_datetime(df_train['date'])\n#df = df_train.set_index('date')\n\n# df_train = df_train.sort_values(by=['country', 'store', 'product', 'date'])\n\n# # Impute NaN with interpolation (grouped by Country, Store, and Product)\n# df_train['num_sold'] = df_train.groupby(['country', 'store', 'product'])['num_sold'].transform(\n#     lambda group: group.interpolate(method='linear',limit=None,limit_direction='both').ffill().bfill()\n# )\n#df['num_sold'].isna().any()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#condition = (df.num_sold.notna()) & (df['country']=='Canada') & (df['store']=='Discount Stickers') & (df['product']=='Holographic Goose')\n#df[condition]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##Get num_sold NaN product store and country\n#t=df[df.num_sold.isna()].groupby(['country','store','product'])['product'].value_counts().to_dict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Country store and product that have NaN\n# for  k,v in t.items():\n#     if v!=0:\n#         print(k,v)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Find Holographic Goose num_sold in each country\nnan_product = df_train[(df_train['product']=='Holographic Goose') & (df_train['num_sold'].notna())].groupby(['country'])['num_sold']\nprint(\"Mean\")\nprint(nan_product.mean())\nprint(\"Median\")\nprint(nan_product.median())\nprint(\"Mode\")\nprint(nan_product.agg(pd.Series.mode))\nprint(\"Min\")\nprint(nan_product.min())\nprint(\"Max\")\nprint(nan_product.max())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Impute Canana NaN with 218.0 \nImpute Kenya NaN with 5.0","metadata":{}},{"cell_type":"code","source":"df_train.dropna(inplace=True)\ndf_test.dropna(inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.isna().any()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Impute Canada with 218\n#df.loc[df['country']=='Canada',['num_sold']]=df.loc[df['country']=='Canada',['num_sold']].fillna(218.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Impute Kenya with 5\n#df.loc[df['country']=='Kenya',['num_sold']]=df.loc[df['country']=='Kenya',['num_sold']].fillna(5.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# nsmode = df_train['num_sold'].mode()\n# df_train['num_sold']=df_train['num_sold'].fillna(nsmode[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Get date back to feature after interpolation\ndf_train=df_train.reset_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['is_holiday']=df_train['is_holiday'].astype('int')\ndf_train['previous_visit']=pd.to_numeric(df_train['previous_visit'].dt.days,errors='coerce')\ndf_train['consec_days']=pd.to_numeric(df_train['consec_days'].dt.days,errors='coerce')\ndf_train['country']=df_train['country'].astype('category')\n\ndf_test['is_holiday']=df_test['is_holiday'].astype('int')\ndf_test['previous_visit']=pd.to_numeric(df_test['previous_visit'].dt.days,errors='coerce')\ndf_test['consec_days']=pd.to_numeric(df_test['consec_days'].dt.days,errors='coerce')\ndf_test['country']=df_test['country'].astype('category')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Split data according to country into\nfrom catboost import Pool\ncountry_train_pool={}\ncountry_valid_pool={}\nfor c in countrylist:\n    df=df_train[df_train['country']==c]\n    train_x, valid_x, train_y, valid_y = train_test_split(df.drop(['id','num_sold','country'],axis=1),df['num_sold'],test_size=0.25)\n    cats=df.select_dtypes('category').columns.to_list()\n    cats.remove('country')\n    country_train_pool[c]=Pool(train_x,label=train_y,cat_features=cats)\n    country_valid_pool[c]=Pool(valid_x,label=valid_y,cat_features=cats)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from functools import partial\n\ndef objective(trial: optuna.Trial, country) -> float:\n    #data, target = load_breast_cancer(return_X_y=True)\n    #train_x, valid_x, train_y, valid_y = train_test_split(df_train.drop(['id','num_sold','date'],axis=1), df_train['num_sold'], test_size=0.25)\n    #cats=df_train.select_dtypes('category').columns\n    \n    param = {\n        \"iterations\":trial.suggest_int(\"iterations\",1000,3000),\n        \"objective\": trial.suggest_categorical(\"objective\", [\"MAPE\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.7, log=True),\n        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"3gb\",\n        \"eval_metric\": \"MAPE\",\n        \"cat_features\": cats,\n    }\n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1, log=True)\n\n    gbm = cb.CatBoostRegressor(**param)\n\n    pruning_callback = CatBoostPruningCallback(trial, \"MAPE\")\n    gbm.fit(\n       # train_x,\n       # train_y,\n       # eval_set=[(valid_x, valid_y)],\n        country_train_pool[c],\n        eval_set=country_valid_pool[c],\n        verbose=0,\n        early_stopping_rounds=300,\n        callbacks=[pruning_callback],\n    )\n\n    # evoke pruning manually.\n    pruning_callback.check_pruned()\n\n    \n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    mape = mean_absolute_percentage_error(valid_y, pred_labels)\n    return mape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from functools import partial \n\ncountry_best_params={}\nfor c in countrylist:\n    objective = partial(objective, country = c)\n    \n    optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"minimize\"\n    )\n    study.optimize(objective, n_trials=100, timeout=600)\n    \n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    \n    print(f'{c} Best trial:')\n    trial = study.best_trial\n    \n    print(\"  Value: {}\".format(trial.value))\n    \n    print(f\"{c}  Params: \")\n    country_best_params[c]=trial.params.copy()\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(study.best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from catboost import Pool,cv\n# cats=df_train.select_dtypes('category').columns\n# features=df_train.drop(['id','num_sold','date'],axis=1).columns\n# train_pool = Pool(data=df_train.drop(['id','num_sold','date'],axis=1),\n#              label=df_train['num_sold'],\n#              cat_features=cats.to_list())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from catboost import Pool,cv\n\n#              #feature_names=features)\n\n\n# # params = {\"iterations\": 1000,\n# #           \"depth\": 4,\n# #           \"loss_function\": \"MAPE\",\n# #           \"verbose\": False}\n\n# scores = cv(train_pool,\n#             study.best_params,\n#            fold_count=5,\n#             verbose=0,\n#            )\n\n# print(scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hyperdict=optuna.importance.get_param_importances(study)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(hyperdict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for p,v in country_best_params.items():\n    print(p)\n    print(v)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Execute this cell in case of loading fitted model to do prediction\ncountry_model={}\ndef LoadModel:\n    Path='kaggle/input/catboost_fitted_models/'\n    countrylist=['Canada','Finland','Italy','Kenya','Norway','Singapore']\n    for c in countrylist:\n        country_model[c]=cb.CatboostRegressor().load_model(f'{Path}/cbfit_{c}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Train model with best param and iteration>1000 before prediction\ncountry_model={}\nfor c in countrylist:\n    country_model[c]=cb.CatBoostRegressor(**country_best_params[c],verbose=0)\n    country_model[c].fit(country_train_pool[c],eval_set=country_valid_pool[c])\n    country_model[c].save_model(f'cbfit_{c}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 3, figsize=(12, 8))\ntestcountrylist=df_test['country'].unique().tolist()\ncountry_predicts={}\nfor i,c in enumerate(testcountrylist):\n    df=df_test[df_test['country']==c]\n    country_predicts[c] = country_model[c].predict(df.drop(['id','country'],axis=1))\n    df['num_sold']=country_predicts[c]\n    df.set_index('date')\n    df['rolling_mean'] = df['num_sold'].rolling(window=30).mean()\n    df['rolling_std'] = df['num_sold'].rolling(window=30).std()\n    df[['num_sold', 'rolling_mean', 'rolling_std']].plot(ax=axes.flat[i],title=f'{c} Rolling Mean and Std Dev')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cdf={}\nfor c in testcountrylist:\n    cid=df_test[df_test['country']==c]['id']\n    cdf[c]=pd.concat([cid,pd.DataFrame(np.round(country_predicts[c],0),columns=['num_sold'])],axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# temp=pd.DataFrame(columns=['id','num_sold'])\n# for c in countrylist:\n#     temp=pd.concat([temp,cdf[c]],ignore_index=True,verify_integrity=True)\ntemp = pd.concat([df.reset_index(drop=True) for df in cdf.values()])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp.to_csv('submission_6country_dropna.csv',index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#pd.concat([df_test['id'],pd.DataFrame(np.round(predicts,0),columns=['num_sold'])],axis=1).reset_index(drop=True).to_csv('submission_impute_dropna_holidays_conseddays.csv',index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}