{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 83035,
          "databundleVersionId": 10369658,
          "sourceType": "competition"
        },
        {
          "sourceId": 7715470,
          "sourceType": "datasetVersion",
          "datasetId": 4505971
        },
        {
          "sourceId": 8754872,
          "sourceType": "datasetVersion",
          "datasetId": 5259309
        },
        {
          "sourceId": 10092657,
          "sourceType": "datasetVersion",
          "datasetId": 4505960
        },
        {
          "sourceId": 85979,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 72240,
          "modelId": 76277
        }
      ],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "papermill": {
      "default_parameters": {},
      "duration": 64.136685,
      "end_time": "2024-02-27T20:52:18.400218",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-02-27T20:51:14.263533",
      "version": "2.5.0"
    },
    "colab": {
      "name": "Gemma 2 Baseline: Generating Essays w/ LLMs ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/warut/kaggle/blob/LLMs-U-Cant-Pls-Em-All/Gemma_2_Baseline_Generating_Essays_w_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "mYg_l_a69Apg"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "llms_you_cant_please_them_all_path = kagglehub.competition_download('llms-you-cant-please-them-all')\n",
        "nischaydnk_immutabledict_path = kagglehub.dataset_download('nischaydnk/immutabledict')\n",
        "fernandosr85_sentencepiece_0_2_0_cp310_cp310_manylinux_path = kagglehub.dataset_download('fernandosr85/sentencepiece-0-2-0-cp310-cp310-manylinux')\n",
        "nischaydnk_gemma_pytorch_path = kagglehub.dataset_download('nischaydnk/gemma-pytorch')\n",
        "google_gemma_2_pytorch_gemma_2_2b_it_1_path = kagglehub.model_download('google/gemma-2/PyTorch/gemma-2-2b-it/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "h8zWT5br9Apu"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma 2 Baseline PyTorch\n",
        "## Generate Random Essays for given Topics with Gemma Models\n",
        "\n",
        "##### This is a baseline notebook in which we will be using Gemma 2 models inference in PyTorch for generating essays.\n",
        "##### You can check out the Github repo of the official PyTorch implementation [here](https://github.com/google/gemma_pytorch).\n",
        "\n",
        "##### You can also use other Gemma Variants including 9B, 27B parameter models. You just need to add different variant from Kaggle models and update the parameters \"GEMMA_MODEL\" , \"MODEL_CONFIG\" & \"MODEL_DIR\".\n",
        "\n",
        "### Note: Since the competition is more than just generating an essay for a given topic, this notebook is just to provide a solution with the help of LLMs."
      ],
      "metadata": {
        "id": "PxGK9W869Apy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.  Installing additional dependencies"
      ],
      "metadata": {
        "id": "eYqpg-xx9Ap3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-index --no-deps /kaggle/input/immutabledict/immutabledict-4.1.0-py3-none-any.whl\n",
        "!pip install --no-index --no-deps /kaggle/input/sentencepiece-0-2-0-cp310-cp310-manylinux/sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "\n",
        "!mkdir /kaggle/working/gemma/\n",
        "!cp /kaggle/input/gemma-pytorch/gemma_pytorch-main/gemma/* /kaggle/working/gemma/"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T22:31:08.850309Z",
          "iopub.execute_input": "2024-12-03T22:31:08.851124Z",
          "iopub.status.idle": "2024-12-03T22:31:12.511225Z",
          "shell.execute_reply.started": "2024-12-03T22:31:08.851087Z",
          "shell.execute_reply": "2024-12-03T22:31:12.510259Z"
        },
        "id": "bEhuled39Ap5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/kaggle/working/\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "ltqhot2P9Ap7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  Import Gemma Modules"
      ],
      "metadata": {
        "id": "TwyvwxIA9Ap8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gemma.config import GemmaConfig, get_model_config\n",
        "from gemma.model import GemmaForCausalLM\n",
        "from gemma.tokenizer import Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "import contextlib\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "random.seed(0)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T23:16:05.573198Z",
          "iopub.execute_input": "2024-12-03T23:16:05.574275Z",
          "iopub.status.idle": "2024-12-03T23:16:05.580507Z",
          "shell.execute_reply.started": "2024-12-03T23:16:05.574213Z",
          "shell.execute_reply": "2024-12-03T23:16:05.57937Z"
        },
        "id": "CrDId8xS9Ap-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Select Gemma Model Variant"
      ],
      "metadata": {
        "id": "C_BsX8b79AqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEMMA_MODEL = '2b-it'\n",
        "DEVICE = 'cuda'\n",
        "MODEL_CONFIG = '2b-v2'\n",
        "MODEL_DIR = \"/kaggle/input/gemma-2/pytorch/gemma-2-2b-it/1\"\n",
        "CKPT_PATH = os.path.join(MODEL_DIR, f'model.ckpt')\n",
        "TOKENIZER_PATH = os.path.join(MODEL_DIR, f'tokenizer.model')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T23:10:05.112103Z",
          "iopub.execute_input": "2024-12-03T23:10:05.113058Z",
          "iopub.status.idle": "2024-12-03T23:10:05.117402Z",
          "shell.execute_reply.started": "2024-12-03T23:10:05.113023Z",
          "shell.execute_reply": "2024-12-03T23:10:05.116451Z"
        },
        "id": "yfNi8GCS9AqB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Loading Model & Model Config"
      ],
      "metadata": {
        "id": "DYx2hNQq9AqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up model config.\n",
        "CONFIG = get_model_config(MODEL_CONFIG)\n",
        "CONFIG.quant = 'quant' in GEMMA_MODEL\n",
        "CONFIG.tokenizer = TOKENIZER_PATH\n",
        "torch.set_default_dtype(CONFIG.get_dtype())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T23:10:13.640526Z",
          "iopub.execute_input": "2024-12-03T23:10:13.640885Z",
          "iopub.status.idle": "2024-12-03T23:10:13.64547Z",
          "shell.execute_reply.started": "2024-12-03T23:10:13.640856Z",
          "shell.execute_reply": "2024-12-03T23:10:13.644543Z"
        },
        "id": "FeEPB4-19AqE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiialize the model and load the weights.\n",
        "device = torch.device(DEVICE)\n",
        "model = GemmaForCausalLM(CONFIG)\n",
        "model.load_weights(CKPT_PATH)\n",
        "model = model.to(device).eval()\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T22:58:19.607345Z",
          "iopub.execute_input": "2024-12-03T22:58:19.607922Z",
          "iopub.status.idle": "2024-12-03T22:58:24.301718Z",
          "shell.execute_reply.started": "2024-12-03T22:58:19.607889Z",
          "shell.execute_reply": "2024-12-03T22:58:24.300848Z"
        },
        "id": "5PluyVtm9AqF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Defining Gemma Chat Template"
      ],
      "metadata": {
        "id": "C_PtgxEv9AqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the prompt format the model expects\n",
        "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T22:59:29.774542Z",
          "iopub.execute_input": "2024-12-03T22:59:29.774911Z",
          "iopub.status.idle": "2024-12-03T22:59:29.779293Z",
          "shell.execute_reply.started": "2024-12-03T22:59:29.774883Z",
          "shell.execute_reply": "2024-12-03T22:59:29.778322Z"
        },
        "id": "xcqMTx0B9AqH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Sample Generation"
      ],
      "metadata": {
        "id": "g5h-guNI9AqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 1024\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, max_seq_length=max_seq_length)\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "input_text = \"What is Kaggle?\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T22:59:41.741276Z",
          "iopub.execute_input": "2024-12-03T22:59:41.741639Z",
          "iopub.status.idle": "2024-12-03T22:59:45.987262Z",
          "shell.execute_reply.started": "2024-12-03T22:59:41.741574Z",
          "shell.execute_reply": "2024-12-03T22:59:45.986529Z"
        },
        "id": "YBM3Rumw9AqI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T22:59:45.988719Z",
          "iopub.execute_input": "2024-12-03T22:59:45.988977Z",
          "iopub.status.idle": "2024-12-03T22:59:45.994272Z",
          "shell.execute_reply.started": "2024-12-03T22:59:45.988952Z",
          "shell.execute_reply": "2024-12-03T22:59:45.993454Z"
        },
        "id": "_kW7CmXB9AqJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print('Chat prompt:\\n', USER_CHAT_TEMPLATE.format(prompt=input_text))\n",
        "\n",
        "results = model.generate(\n",
        "    USER_CHAT_TEMPLATE.format(prompt=input_text),\n",
        "    device=DEVICE,\n",
        "    output_len=128,\n",
        ")\n",
        "print(results)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T22:59:49.697744Z",
          "iopub.execute_input": "2024-12-03T22:59:49.698437Z",
          "iopub.status.idle": "2024-12-03T22:59:55.446968Z",
          "shell.execute_reply.started": "2024-12-03T22:59:49.698406Z",
          "shell.execute_reply": "2024-12-03T22:59:55.446128Z"
        },
        "id": "okVSzBMH9AqK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Design your own Prompt"
      ],
      "metadata": {
        "id": "knrEwEe89AqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_for_llm = (\n",
        "    \"<start_of_turn>user\\nGenerate an essay for the following topic with no more than 100 words: {topic_name}.\"\n",
        "    \"Pay attention to detail, clarity, and overall quality in your generated essay.\"\n",
        "    \"<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-03T23:05:13.781074Z",
          "iopub.execute_input": "2024-12-03T23:05:13.781934Z",
          "iopub.status.idle": "2024-12-03T23:05:13.785915Z",
          "shell.execute_reply.started": "2024-12-03T23:05:13.781898Z",
          "shell.execute_reply": "2024-12-03T23:05:13.784998Z"
        },
        "papermill": {
          "duration": 0.016016,
          "end_time": "2024-02-27T20:52:12.209684",
          "exception": false,
          "start_time": "2024-02-27T20:52:12.193668",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "YlWfBvDH9AqL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/test.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-03T23:00:09.290305Z",
          "iopub.execute_input": "2024-12-03T23:00:09.29117Z",
          "iopub.status.idle": "2024-12-03T23:00:09.298341Z",
          "shell.execute_reply.started": "2024-12-03T23:00:09.291136Z",
          "shell.execute_reply": "2024-12-03T23:00:09.297644Z"
        },
        "papermill": {
          "duration": 0.046216,
          "end_time": "2024-02-27T20:52:12.264417",
          "exception": false,
          "start_time": "2024-02-27T20:52:12.218201",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "lhGoTSqo9AqL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T23:00:12.828225Z",
          "iopub.execute_input": "2024-12-03T23:00:12.829003Z",
          "iopub.status.idle": "2024-12-03T23:00:12.836469Z",
          "shell.execute_reply.started": "2024-12-03T23:00:12.828969Z",
          "shell.execute_reply": "2024-12-03T23:00:12.835666Z"
        },
        "id": "QCTIa97-9AqM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-03T23:00:20.558725Z",
          "iopub.execute_input": "2024-12-03T23:00:20.559038Z",
          "iopub.status.idle": "2024-12-03T23:00:20.569674Z",
          "shell.execute_reply.started": "2024-12-03T23:00:20.559014Z",
          "shell.execute_reply": "2024-12-03T23:00:20.568867Z"
        },
        "papermill": {
          "duration": 0.023977,
          "end_time": "2024-02-27T20:52:12.296966",
          "exception": false,
          "start_time": "2024-02-27T20:52:12.272989",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "3nkcggT89AqN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test.loc[0,'topic']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-03T23:00:30.349251Z",
          "iopub.execute_input": "2024-12-03T23:00:30.349831Z",
          "iopub.status.idle": "2024-12-03T23:00:30.355524Z",
          "shell.execute_reply.started": "2024-12-03T23:00:30.349796Z",
          "shell.execute_reply": "2024-12-03T23:00:30.354621Z"
        },
        "papermill": {
          "duration": 0.022089,
          "end_time": "2024-02-27T20:52:12.327421",
          "exception": false,
          "start_time": "2024-02-27T20:52:12.305332",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "iOgHXXNG9AqN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-03T23:00:42.612773Z",
          "iopub.execute_input": "2024-12-03T23:00:42.613489Z",
          "iopub.status.idle": "2024-12-03T23:00:42.621021Z",
          "shell.execute_reply.started": "2024-12-03T23:00:42.613457Z",
          "shell.execute_reply": "2024-12-03T23:00:42.620099Z"
        },
        "papermill": {
          "duration": 0.019735,
          "end_time": "2024-02-27T20:52:12.354767",
          "exception": false,
          "start_time": "2024-02-27T20:52:12.335032",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "vfjmI-rW9AqO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Basic Baseline\n",
        "#### Here we will try to use Gemma model for generating essays relevant to the provided topics in test csv."
      ],
      "metadata": {
        "id": "cscCpLga9AqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i in range(len(test)):\n",
        "    topic = test.loc[i, 'topic']\n",
        "    gen_essay = model.generate(\n",
        "        prompt_for_llm.format(topic_name=topic),\n",
        "        device=device,\n",
        "        output_len=128,\n",
        "    )\n",
        "    predictions.append(gen_essay)\n",
        "\n",
        "    if i<=2:\n",
        "        print('Topic: ', topic)\n",
        "        print('Generated Essay: ', gen_essay)\n",
        "        print('\\n\\n***********************\\n\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-03T23:08:22.981869Z",
          "iopub.execute_input": "2024-12-03T23:08:22.982183Z",
          "iopub.status.idle": "2024-12-03T23:08:40.44527Z",
          "shell.execute_reply.started": "2024-12-03T23:08:22.982155Z",
          "shell.execute_reply": "2024-12-03T23:08:40.444391Z"
        },
        "papermill": {
          "duration": 4.806177,
          "end_time": "2024-02-27T20:52:17.169674",
          "exception": false,
          "start_time": "2024-02-27T20:52:12.363497",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "V2_Y721C9AqO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-03T23:08:40.447102Z",
          "iopub.execute_input": "2024-12-03T23:08:40.448027Z",
          "iopub.status.idle": "2024-12-03T23:08:40.453729Z",
          "shell.execute_reply.started": "2024-12-03T23:08:40.447984Z",
          "shell.execute_reply": "2024-12-03T23:08:40.452658Z"
        },
        "papermill": {
          "duration": 0.015209,
          "end_time": "2024-02-27T20:52:17.192106",
          "exception": false,
          "start_time": "2024-02-27T20:52:17.176897",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "2tNm_jYm9AqP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub['essay'] = predictions\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-03T23:08:40.45468Z",
          "iopub.execute_input": "2024-12-03T23:08:40.454997Z",
          "iopub.status.idle": "2024-12-03T23:08:40.471218Z",
          "shell.execute_reply.started": "2024-12-03T23:08:40.454972Z",
          "shell.execute_reply": "2024-12-03T23:08:40.470482Z"
        },
        "papermill": {
          "duration": 0.013597,
          "end_time": "2024-02-27T20:52:17.212446",
          "exception": false,
          "start_time": "2024-02-27T20:52:17.198849",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "R8IC5rEW9AqQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-03T23:08:40.472765Z",
          "iopub.execute_input": "2024-12-03T23:08:40.473011Z",
          "iopub.status.idle": "2024-12-03T23:08:40.486158Z",
          "shell.execute_reply.started": "2024-12-03T23:08:40.472986Z",
          "shell.execute_reply": "2024-12-03T23:08:40.485275Z"
        },
        "papermill": {
          "duration": 0.016846,
          "end_time": "2024-02-27T20:52:17.236046",
          "exception": false,
          "start_time": "2024-02-27T20:52:17.2192",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "mPsoLTNz9AqQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub.to_csv('submission.csv',index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-03T23:08:40.487317Z",
          "iopub.execute_input": "2024-12-03T23:08:40.487755Z",
          "iopub.status.idle": "2024-12-03T23:08:40.500162Z",
          "shell.execute_reply.started": "2024-12-03T23:08:40.487729Z",
          "shell.execute_reply": "2024-12-03T23:08:40.499164Z"
        },
        "papermill": {
          "duration": 0.016296,
          "end_time": "2024-02-27T20:52:17.259257",
          "exception": false,
          "start_time": "2024-02-27T20:52:17.242961",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "dq8oGzt09AqR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "papermill": {
          "duration": 0.006777,
          "end_time": "2024-02-27T20:52:17.27334",
          "exception": false,
          "start_time": "2024-02-27T20:52:17.266563",
          "status": "completed"
        },
        "tags": [],
        "id": "PYBaizGm9AqS"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}