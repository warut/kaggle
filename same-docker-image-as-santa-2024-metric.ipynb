{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This notebook uses the same Docker image as the Santa metric","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:30:36.149231Z","iopub.execute_input":"2025-01-01T08:30:36.149562Z","iopub.status.idle":"2025-01-01T08:30:36.153481Z","shell.execute_reply.started":"2025-01-01T08:30:36.149533Z","shell.execute_reply":"2025-01-01T08:30:36.152562Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install bitsandbytes accelerate\n!pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:38:04.821237Z","iopub.execute_input":"2025-01-01T16:38:04.821550Z","iopub.status.idle":"2025-01-01T16:38:36.260142Z","shell.execute_reply.started":"2025-01-01T16:38:04.821512Z","shell.execute_reply":"2025-01-01T16:38:36.259303Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.0\n    Uninstalling tokenizers-0.20.0:\n      Successfully uninstalled tokenizers-0.20.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed tokenizers-0.21.0 transformers-4.47.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\ntorch.backends.cuda.enable_mem_efficient_sdp(False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:38:42.656751Z","iopub.execute_input":"2025-01-01T16:38:42.657118Z","iopub.status.idle":"2025-01-01T16:38:46.228959Z","shell.execute_reply.started":"2025-01-01T16:38:42.657083Z","shell.execute_reply":"2025-01-01T16:38:46.228304Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\"\"\"Evaluation metric for Santa 2024.\"\"\"\n\nimport gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n        Path to the serialized LLM.\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    clear_mem : bool, default=False\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], debug=False\n    ) -> Union[float, List[float]]:\n        \"\"\"\n        Calculates the perplexity of given texts.\n\n        Parameters\n        ----------\n        input_texts : str or list of str\n            A single string or a list of strings.\n\n        batch_size : int, default=None\n            Batch size for processing. Defaults to the number of input texts.\n\n        debug : bool, default=False\n            Print debugging information.\n\n        Returns\n        -------\n        float or list of float\n            A single perplexity value if input is a single string,\n            or a list of perplexity values if input is a list of strings.\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n        >>> scorer = PerplexityCalculator(model_path=model_path)\n\n        >>> submission = pd.DataFrame({\n        ...     'id': [0, 1, 2],\n        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n        ... })\n        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        >>> perplexities[0] < perplexities[1]\n        True\n        >>> perplexities[2] < perplexities[0]\n        True\n\n        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n        >>> all(p > 0 for p in perplexities)\n        True\n\n        >>> scorer.clear_gpu_memory()\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n        with torch.no_grad():\n            # Process each sequence independently\n            for text in input_texts:\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                # Calculate average loss\n                sequence_loss = loss.sum() / len(loss)\n                loss_list.append(sequence_loss.cpu().item())\n\n                # Debug output\n                if debug:\n                    print(f\"\\nProcessing: '{text}'\")\n                    print(f\"With special tokens: '{text_with_special}'\")\n                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                    print(f\"Individual losses: {loss.tolist()}\")\n                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        if debug:\n            print(\"\\nFinal perplexities:\")\n            for text, perp in zip(input_texts, ppl):\n                print(f\"Text: '{text}'\")\n                print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:38:49.699314Z","iopub.execute_input":"2025-01-01T16:38:49.699771Z","iopub.status.idle":"2025-01-01T16:38:50.625664Z","shell.execute_reply.started":"2025-01-01T16:38:49.699739Z","shell.execute_reply":"2025-01-01T16:38:50.624732Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:38:59.854603Z","iopub.execute_input":"2025-01-01T16:38:59.855256Z","iopub.status.idle":"2025-01-01T16:38:59.864474Z","shell.execute_reply.started":"2025-01-01T16:38:59.855218Z","shell.execute_reply":"2025-01-01T16:38:59.863708Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model.safetensors.index.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00001-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00003-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00002-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00007-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/README.md\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00008-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer_config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00005-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00006-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/special_tokens_map.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/.gitattributes\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer.model\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00004-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/generation_config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/transformers/transformers-4.42.0.dev0-py3-none-any.whl\n/kaggle/input/santa-2024/sample_submission.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/santa-2024/sample_submission.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:39:06.779215Z","iopub.execute_input":"2025-01-01T16:39:06.780144Z","iopub.status.idle":"2025-01-01T16:39:06.792043Z","shell.execute_reply.started":"2025-01-01T16:39:06.780097Z","shell.execute_reply":"2025-01-01T16:39:06.791200Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"sets=[]\n\nfor i in range(6):\n    sets.append(set(df.loc[i,'text'].split()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:39:10.587437Z","iopub.execute_input":"2025-01-01T16:39:10.588291Z","iopub.status.idle":"2025-01-01T16:39:10.596450Z","shell.execute_reply.started":"2025-01-01T16:39:10.588254Z","shell.execute_reply":"2025-01-01T16:39:10.595560Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"for i in range(5):\n   if sets[i].issubset(sets[5]):\n       print(f'set {i} is subset of set5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:39:14.436855Z","iopub.execute_input":"2025-01-01T16:39:14.437212Z","iopub.status.idle":"2025-01-01T16:39:14.442441Z","shell.execute_reply.started":"2025-01-01T16:39:14.437182Z","shell.execute_reply":"2025-01-01T16:39:14.441541Z"}},"outputs":[{"name":"stdout","text":"set 0 is subset of set5\nset 1 is subset of set5\nset 2 is subset of set5\nset 3 is subset of set5\nset 4 is subset of set5\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"for i in range(1,6):\n    if sets[0].issubset(sets[i]):\n      print(f'set 0 is subset of set {i}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:39:17.739964Z","iopub.execute_input":"2025-01-01T16:39:17.740660Z","iopub.status.idle":"2025-01-01T16:39:17.745064Z","shell.execute_reply.started":"2025-01-01T16:39:17.740627Z","shell.execute_reply":"2025-01-01T16:39:17.744217Z"}},"outputs":[{"name":"stdout","text":"set 0 is subset of set 1\nset 0 is subset of set 5\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"for i in range(6):\n    if i!=1 and sets[1].issubset(sets[i]):\n      print(f'set 1 is subset of set {i}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:39:20.788339Z","iopub.execute_input":"2025-01-01T16:39:20.788767Z","iopub.status.idle":"2025-01-01T16:39:20.793558Z","shell.execute_reply.started":"2025-01-01T16:39:20.788738Z","shell.execute_reply":"2025-01-01T16:39:20.792619Z"}},"outputs":[{"name":"stdout","text":"set 1 is subset of set 5\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"for i in range(6):\n    if i!=2 and sets[2].issubset(sets[i]):\n      print(f'set 2 is subset of set {i}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:39:23.588828Z","iopub.execute_input":"2025-01-01T16:39:23.589403Z","iopub.status.idle":"2025-01-01T16:39:23.593709Z","shell.execute_reply.started":"2025-01-01T16:39:23.589369Z","shell.execute_reply":"2025-01-01T16:39:23.592893Z"}},"outputs":[{"name":"stdout","text":"set 2 is subset of set 3\nset 2 is subset of set 5\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"for i in range(6):\n    if i!=3 and sets[3].issubset(sets[i]):\n      print(f'set 3 is subset of set {i}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:39:26.448072Z","iopub.execute_input":"2025-01-01T16:39:26.448429Z","iopub.status.idle":"2025-01-01T16:39:26.453541Z","shell.execute_reply.started":"2025-01-01T16:39:26.448396Z","shell.execute_reply":"2025-01-01T16:39:26.452531Z"}},"outputs":[{"name":"stdout","text":"set 3 is subset of set 5\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"sets[4].issubset(sets[5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:39:29.602610Z","iopub.execute_input":"2025-01-01T16:39:29.602957Z","iopub.status.idle":"2025-01-01T16:39:29.609254Z","shell.execute_reply.started":"2025-01-01T16:39:29.602912Z","shell.execute_reply":"2025-01-01T16:39:29.608451Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"lensets=[len(sets[i]) for i in range(6)]\nprint(f'len of sets {lensets}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:39:38.689295Z","iopub.execute_input":"2025-01-01T16:39:38.689665Z","iopub.status.idle":"2025-01-01T16:39:38.694628Z","shell.execute_reply.started":"2025-01-01T16:39:38.689634Z","shell.execute_reply":"2025-01-01T16:39:38.693631Z"}},"outputs":[{"name":"stdout","text":"len of sets [10, 20, 20, 29, 50, 89]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"lenlists = [len(df.loc[i,'text'].split()) for i in range(6)]\nprint(f'len of lists {lenlists}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:30:36.397618Z","iopub.execute_input":"2025-01-01T08:30:36.397895Z","iopub.status.idle":"2025-01-01T08:30:36.402854Z","shell.execute_reply.started":"2025-01-01T08:30:36.397872Z","shell.execute_reply":"2025-01-01T08:30:36.401915Z"}},"outputs":[{"name":"stdout","text":"len of lists [10, 20, 20, 30, 50, 100]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"temp=[]\nfor w in df.loc[3,'text'].split():\n    if w in temp:\n        print(f'the dup word in set 3 is {w}')\n    else:\n        temp.append(w)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:30:36.404497Z","iopub.execute_input":"2025-01-01T08:30:36.404828Z","iopub.status.idle":"2025-01-01T08:30:36.412604Z","shell.execute_reply.started":"2025-01-01T08:30:36.404794Z","shell.execute_reply":"2025-01-01T08:30:36.411885Z"}},"outputs":[{"name":"stdout","text":"the dup word in set 3 is cheer\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"temp=[]\nfor w in df.loc[5,'text'].split():\n    if w in temp:\n        print(f'the dup word in set 5 is {w}')\n    else:\n        temp.append(w)\nprint(f'len of temp {len(temp)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:30:36.778209Z","iopub.execute_input":"2025-01-01T08:30:36.778874Z","iopub.status.idle":"2025-01-01T08:30:36.783934Z","shell.execute_reply.started":"2025-01-01T08:30:36.778836Z","shell.execute_reply":"2025-01-01T08:30:36.783038Z"}},"outputs":[{"name":"stdout","text":"the dup word in set 5 is chimney\nthe dup word in set 5 is ornament\nthe dup word in set 5 is cheer\nthe dup word in set 5 is and\nthe dup word in set 5 is the\nthe dup word in set 5 is workshop\nthe dup word in set 5 is fireplace\nthe dup word in set 5 is night\nthe dup word in set 5 is the\nthe dup word in set 5 is of\nthe dup word in set 5 is and\nlen of temp 89\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nmodel_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\nsolution = pd.DataFrame({'id': [0, 1],\n                         'text': [\"this is a normal english sentence\",\"the quick brown fox jumps over the lazy dog\"]}\n                       )\nsubmission = pd.DataFrame({'id': [0, 1],\n                           'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"\n                                   ]})\nscore(solution, submission, 'id', model_path=model_path, clear_mem=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:40:19.201429Z","iopub.execute_input":"2025-01-01T16:40:19.202162Z","iopub.status.idle":"2025-01-01T16:43:05.136389Z","shell.execute_reply.started":"2025-01-01T16:40:19.202127Z","shell.execute_reply":"2025-01-01T16:43:05.135352Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5942d96d0737470aaa0af956cb6e66d3"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"14840.77285767256"},"metadata":{}}],"execution_count":15}]}